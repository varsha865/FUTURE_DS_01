{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "6pwvtuwdgYTz",
        "outputId": "9c859af0-9154-4099-90cf-26556a519d96"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'drive/My Drive/Projects/Twitter Sentiment/train_tweet.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a5cb0352d800>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'drive/My Drive/Projects/Twitter Sentiment/train_tweet.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'drive/My Drive/Projects/Twitter Sentiment/test_tweets.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'drive/My Drive/Projects/Twitter Sentiment/train_tweet.csv'"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Twitter Sentiment.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1uULaa-NzuJENkwmGA5vPjZEb2m5Z-j4b\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "\n",
        "train = pd.read_csv('drive/My Drive/Projects/Twitter Sentiment/train_tweet.csv')\n",
        "test = pd.read_csv('drive/My Drive/Projects/Twitter Sentiment/test_tweets.csv')\n",
        "\n",
        "print(train.shape)\n",
        "print(test.shape)\n",
        "\n",
        "train.head()\n",
        "\n",
        "test.head()\n",
        "\n",
        "train.isnull().any()\n",
        "test.isnull().any()\n",
        "\n",
        "# checking out the negative comments from the train set\n",
        "\n",
        "train[train['label'] == 0].head(10)\n",
        "\n",
        "# checking out the postive comments from the train set\n",
        "\n",
        "train[train['label'] == 1].head(10)\n",
        "\n",
        "train['label'].value_counts().plot.bar(color = 'pink', figsize = (6, 4))\n",
        "\n",
        "# checking the distribution of tweets in the data\n",
        "\n",
        "length_train = train['tweet'].str.len().plot.hist(color = 'pink', figsize = (6, 4))\n",
        "length_test = test['tweet'].str.len().plot.hist(color = 'orange', figsize = (6, 4))\n",
        "\n",
        "# adding a column to represent the length of the tweet\n",
        "\n",
        "train['len'] = train['tweet'].str.len()\n",
        "test['len'] = test['tweet'].str.len()\n",
        "\n",
        "train.head(10)\n",
        "\n",
        "train.groupby('label').describe()\n",
        "\n",
        "train.groupby('len').mean()['label'].plot.hist(color = 'black', figsize = (6, 4),)\n",
        "plt.title('variation of length')\n",
        "plt.xlabel('Length')\n",
        "plt.show()\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "cv = CountVectorizer(stop_words = 'english')\n",
        "words = cv.fit_transform(train.tweet)\n",
        "\n",
        "sum_words = words.sum(axis=0)\n",
        "\n",
        "words_freq = [(word, sum_words[0, i]) for word, i in cv.vocabulary_.items()]\n",
        "words_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\n",
        "\n",
        "frequency = pd.DataFrame(words_freq, columns=['word', 'freq'])\n",
        "\n",
        "frequency.head(30).plot(x='word', y='freq', kind='bar', figsize=(15, 7), color = 'blue')\n",
        "plt.title(\"Most Frequently Occuring Words - Top 30\")\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "wordcloud = WordCloud(background_color = 'white', width = 1000, height = 1000).generate_from_frequencies(dict(words_freq))\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(wordcloud)\n",
        "plt.title(\"WordCloud - Vocabulary from Reviews\", fontsize = 22)\n",
        "\n",
        "normal_words =' '.join([text for text in train['tweet'][train['label'] == 0]])\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=500, random_state = 0, max_font_size = 110).generate(normal_words)\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.title('The Neutral Words')\n",
        "plt.show()\n",
        "\n",
        "negative_words =' '.join([text for text in train['tweet'][train['label'] == 1]])\n",
        "\n",
        "wordcloud = WordCloud(background_color = 'cyan', width=800, height=500, random_state = 0, max_font_size = 110).generate(negative_words)\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.title('The Negative Words')\n",
        "plt.show()\n",
        "\n",
        "# collecting the hashtags\n",
        "\n",
        "def hashtag_extract(x):\n",
        "    hashtags = []\n",
        "\n",
        "    for i in x:\n",
        "        ht = re.findall(r\"#(\\w+)\", i)\n",
        "        hashtags.append(ht)\n",
        "\n",
        "    return hashtags\n",
        "\n",
        "# extracting hashtags from non racist/sexist tweets\n",
        "HT_regular = hashtag_extract(train['tweet'][train['label'] == 0])\n",
        "\n",
        "# extracting hashtags from racist/sexist tweets\n",
        "HT_negative = hashtag_extract(train['tweet'][train['label'] == 1])\n",
        "\n",
        "# unnesting list\n",
        "HT_regular = sum(HT_regular,[])\n",
        "HT_negative = sum(HT_negative,[])\n",
        "\n",
        "a = nltk.FreqDist(HT_regular)\n",
        "d = pd.DataFrame({'Hashtag': list(a.keys()),\n",
        "                  'Count': list(a.values())})\n",
        "\n",
        "# selecting top 20 most frequent hashtags\n",
        "d = d.nlargest(columns=\"Count\", n = 20)\n",
        "plt.figure(figsize=(16,5))\n",
        "ax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\n",
        "ax.set(ylabel = 'Count')\n",
        "plt.show()\n",
        "\n",
        "a = nltk.FreqDist(HT_negative)\n",
        "d = pd.DataFrame({'Hashtag': list(a.keys()),\n",
        "                  'Count': list(a.values())})\n",
        "\n",
        "# selecting top 20 most frequent hashtags\n",
        "d = d.nlargest(columns=\"Count\", n = 20)\n",
        "plt.figure(figsize=(16,5))\n",
        "ax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\n",
        "ax.set(ylabel = 'Count')\n",
        "plt.show()\n",
        "\n",
        "# tokenizing the words present in the training set\n",
        "tokenized_tweet = train['tweet'].apply(lambda x: x.split())\n",
        "\n",
        "# importing gensim\n",
        "import gensim\n",
        "\n",
        "# creating a word to vector model\n",
        "model_w2v = gensim.models.Word2Vec(\n",
        "            tokenized_tweet,\n",
        "            size=200, # desired no. of features/independent variables\n",
        "            window=5, # context window size\n",
        "            min_count=2,\n",
        "            sg = 1, # 1 for skip-gram model\n",
        "            hs = 0,\n",
        "            negative = 10, # for negative sampling\n",
        "            workers= 2, # no.of cores\n",
        "            seed = 34)\n",
        "\n",
        "model_w2v.train(tokenized_tweet, total_examples= len(train['tweet']), epochs=20)\n",
        "\n",
        "model_w2v.wv.most_similar(positive = \"dinner\")\n",
        "\n",
        "model_w2v.wv.most_similar(positive = \"cancer\")\n",
        "\n",
        "model_w2v.wv.most_similar(positive = \"apple\")\n",
        "\n",
        "model_w2v.wv.most_similar(negative = \"hate\")\n",
        "\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas(desc=\"progress-bar\")\n",
        "from gensim.models.doc2vec import LabeledSentence\n",
        "\n",
        "def add_label(twt):\n",
        "    output = []\n",
        "    for i, s in zip(twt.index, twt):\n",
        "        output.append(LabeledSentence(s, [\"tweet_\" + str(i)]))\n",
        "    return output\n",
        "\n",
        "# label all the tweets\n",
        "labeled_tweets = add_label(tokenized_tweet)\n",
        "\n",
        "labeled_tweets[:6]\n",
        "\n",
        "# removing unwanted patterns from the data\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "train_corpus = []\n",
        "\n",
        "for i in range(0, 31962):\n",
        "  review = re.sub('[^a-zA-Z]', ' ', train['tweet'][i])\n",
        "  review = review.lower()\n",
        "  review = review.split()\n",
        "\n",
        "  ps = PorterStemmer()\n",
        "\n",
        "  # stemming\n",
        "  review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "\n",
        "  # joining them back with space\n",
        "  review = ' '.join(review)\n",
        "  train_corpus.append(review)\n",
        "\n",
        "test_corpus = []\n",
        "\n",
        "for i in range(0, 17197):\n",
        "  review = re.sub('[^a-zA-Z]', ' ', test['tweet'][i])\n",
        "  review = review.lower()\n",
        "  review = review.split()\n",
        "\n",
        "  ps = PorterStemmer()\n",
        "\n",
        "  # stemming\n",
        "  review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "\n",
        "  # joining them back with space\n",
        "  review = ' '.join(review)\n",
        "  test_corpus.append(review)\n",
        "\n",
        "# creating bag of words\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer(max_features = 2500)\n",
        "x = cv.fit_transform(train_corpus).toarray()\n",
        "y = train.iloc[:, 1]\n",
        "\n",
        "print(x.shape)\n",
        "print(y.shape)\n",
        "\n",
        "# creating bag of words\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer(max_features = 2500)\n",
        "x_test = cv.fit_transform(test_corpus).toarray()\n",
        "\n",
        "print(x_test.shape)\n",
        "\n",
        "# splitting the training data into train and valid sets\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size = 0.25, random_state = 42)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_valid.shape)\n",
        "print(y_train.shape)\n",
        "print(y_valid.shape)\n",
        "\n",
        "# standardization\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "sc = StandardScaler()\n",
        "\n",
        "x_train = sc.fit_transform(x_train)\n",
        "x_valid = sc.transform(x_valid)\n",
        "x_test = sc.transform(x_test)\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "model = RandomForestClassifier()\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "y_pred = model.predict(x_valid)\n",
        "\n",
        "print(\"Training Accuracy :\", model.score(x_train, y_train))\n",
        "print(\"Validation Accuracy :\", model.score(x_valid, y_valid))\n",
        "\n",
        "# calculating the f1 score for the validation set\n",
        "print(\"F1 score :\", f1_score(y_valid, y_pred))\n",
        "\n",
        "# confusion matrix\n",
        "cm = confusion_matrix(y_valid, y_pred)\n",
        "print(cm)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "y_pred = model.predict(x_valid)\n",
        "\n",
        "print(\"Training Accuracy :\", model.score(x_train, y_train))\n",
        "print(\"Validation Accuracy :\", model.score(x_valid, y_valid))\n",
        "\n",
        "# calculating the f1 score for the validation set\n",
        "print(\"f1 score :\", f1_score(y_valid, y_pred))\n",
        "\n",
        "# confusion matrix\n",
        "cm = confusion_matrix(y_valid, y_pred)\n",
        "print(cm)\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "model = DecisionTreeClassifier()\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "y_pred = model.predict(x_valid)\n",
        "\n",
        "print(\"Training Accuracy :\", model.score(x_train, y_train))\n",
        "print(\"Validation Accuracy :\", model.score(x_valid, y_valid))\n",
        "\n",
        "# calculating the f1 score for the validation set\n",
        "print(\"f1 score :\", f1_score(y_valid, y_pred))\n",
        "\n",
        "# confusion matrix\n",
        "cm = confusion_matrix(y_valid, y_pred)\n",
        "print(cm)\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "model = SVC()\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "y_pred = model.predict(x_valid)\n",
        "\n",
        "print(\"Training Accuracy :\", model.score(x_train, y_train))\n",
        "print(\"Validation Accuracy :\", model.score(x_valid, y_valid))\n",
        "\n",
        "# calculating the f1 score for the validation set\n",
        "print(\"f1 score :\", f1_score(y_valid, y_pred))\n",
        "\n",
        "# confusion matrix\n",
        "cm = confusion_matrix(y_valid, y_pred)\n",
        "print(cm)\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "model = XGBClassifier()\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "y_pred = model.predict(x_valid)\n",
        "\n",
        "print(\"Training Accuracy :\", model.score(x_train, y_train))\n",
        "print(\"Validation Accuracy :\", model.score(x_valid, y_valid))\n",
        "\n",
        "# calculating the f1 score for the validation set\n",
        "print(\"f1 score :\", f1_score(y_valid, y_pred))\n",
        "\n",
        "# confusion matrix\n",
        "cm = confusion_matrix(y_valid, y_pred)\n",
        "print(cm)"
      ]
    }
  ]
}